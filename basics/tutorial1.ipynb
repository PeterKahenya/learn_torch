{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
    "testing_data = datasets.FashionMNIST(root=\"data\",train=False,download=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "training_data_loader = DataLoader(training_data,batch_size=batch_size)\n",
    "testing_data_loader = DataLoader(testing_data,batch_size=batch_size)\n",
    "\n",
    "for X,y in testing_data_loader:\n",
    "    print(f\"Shape of X[N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model:nn.Module, loss_fn:nn.CrossEntropyLoss, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # sets module in training mode\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        #predict and get the error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        #backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model:nn.Module, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # sets module in evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299724  [   64/60000]\n",
      "loss: 2.291102  [ 6464/60000]\n",
      "loss: 2.274094  [12864/60000]\n",
      "loss: 2.266974  [19264/60000]\n",
      "loss: 2.249399  [25664/60000]\n",
      "loss: 2.226276  [32064/60000]\n",
      "loss: 2.227283  [38464/60000]\n",
      "loss: 2.192272  [44864/60000]\n",
      "loss: 2.183540  [51264/60000]\n",
      "loss: 2.154141  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 2.157632 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.164852  [   64/60000]\n",
      "loss: 2.162076  [ 6464/60000]\n",
      "loss: 2.105527  [12864/60000]\n",
      "loss: 2.117648  [19264/60000]\n",
      "loss: 2.067086  [25664/60000]\n",
      "loss: 2.004032  [32064/60000]\n",
      "loss: 2.031563  [38464/60000]\n",
      "loss: 1.949140  [44864/60000]\n",
      "loss: 1.950905  [51264/60000]\n",
      "loss: 1.876613  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.888081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.919366  [   64/60000]\n",
      "loss: 1.898294  [ 6464/60000]\n",
      "loss: 1.781851  [12864/60000]\n",
      "loss: 1.815315  [19264/60000]\n",
      "loss: 1.703098  [25664/60000]\n",
      "loss: 1.652561  [32064/60000]\n",
      "loss: 1.673015  [38464/60000]\n",
      "loss: 1.574717  [44864/60000]\n",
      "loss: 1.599266  [51264/60000]\n",
      "loss: 1.492239  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.519422 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.586339  [   64/60000]\n",
      "loss: 1.558870  [ 6464/60000]\n",
      "loss: 1.408160  [12864/60000]\n",
      "loss: 1.471235  [19264/60000]\n",
      "loss: 1.357088  [25664/60000]\n",
      "loss: 1.350477  [32064/60000]\n",
      "loss: 1.362115  [38464/60000]\n",
      "loss: 1.286204  [44864/60000]\n",
      "loss: 1.318475  [51264/60000]\n",
      "loss: 1.224158  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.253069 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.329184  [   64/60000]\n",
      "loss: 1.318064  [ 6464/60000]\n",
      "loss: 1.149576  [12864/60000]\n",
      "loss: 1.248571  [19264/60000]\n",
      "loss: 1.131301  [25664/60000]\n",
      "loss: 1.151880  [32064/60000]\n",
      "loss: 1.170435  [38464/60000]\n",
      "loss: 1.105902  [44864/60000]\n",
      "loss: 1.141647  [51264/60000]\n",
      "loss: 1.066161  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.087462 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    \n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data_loader, model, loss_fn, optimizer)\n",
    "    test(testing_data_loader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4318, -2.6873, -1.1547, -2.0186, -1.0958,  2.2845, -1.1788,  2.5103,\n",
      "          1.6418,  3.0134]])\n",
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x,y = testing_data[0][0], testing_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(pred)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Are similar to numpy ndarrays only these can be run on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "<class 'torch.Tensor'>\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6572, 0.5243],\n",
      "        [0.8393, 0.3537]], dtype=torch.float64) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6256, 0.8364, 0.5874],\n",
      "        [0.9177, 0.1581, 0.5875]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "data_tensor = torch.tensor(data)\n",
    "print(data_tensor)\n",
    "print(type(data_tensor))\n",
    "np_ndarray = np.array(data)\n",
    "data_tensor = torch.from_numpy(np_ndarray)\n",
    "print(data_tensor)\n",
    "print(type(data_tensor))\n",
    "x_ones = torch.ones_like(data_tensor)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(data_tensor, dtype=torch.float64)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7562, 0.1546, 0.7825, 0.6120],\n",
      "        [0.5661, 0.1221, 0.5718, 0.7756],\n",
      "        [0.2494, 0.9050, 0.3983, 0.1894]], dtype=torch.float64)\n",
      "shape is torch.Size([3, 4])\n",
      "datatype is torch.float64\n",
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((3,4),dtype=torch.float64)\n",
    "print(tensor)\n",
    "print(f\"shape is {tensor.shape}\")\n",
    "print(f\"datatype is {tensor.dtype}\")\n",
    "print(f\"device is {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([5., 5., 5., 5., 5.])\n",
      "n: [5. 5. 5. 5. 5.]\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "tensor = torch.rand(3,4)\n",
    "# print(tensor)\n",
    "first_row = tensor[0]\n",
    "# print(first_row)\n",
    "first_row = tensor[0,:]\n",
    "# print(first_row)\n",
    "first_column = tensor[:,0]\n",
    "# print(first_column)\n",
    "last_column = tensor[:,-1]\n",
    "# print(last_column)\n",
    "last_column = tensor[...,-1]\n",
    "# print(last_column)\n",
    "t1 = torch.cat([tensor, tensor], dim=1)\n",
    "# print(t1)\n",
    "t2 = torch.cat([tensor, tensor], dim=0)\n",
    "# print(t2)\n",
    "\n",
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "# print(y1)\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "# print(y2)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "# print(y3)\n",
    "\n",
    "y3_mult = torch.matmul(tensor, tensor.T, out=y3)\n",
    "# print(y3_mult)\n",
    "\n",
    "\n",
    "# # This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "# print(z1)\n",
    "z2 = tensor.mul(tensor)\n",
    "# print(z2)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "# print(z3)\n",
    "# torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "# print(agg_item, type(agg_item))\n",
    "# print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "# print(tensor)\n",
    "\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "t.add_(4)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import uuid\n",
    "\n",
    "labels_map = {\n",
    "    0: \"TShirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"AnkleBoot\",\n",
    "}\n",
    "\n",
    "train_data = pd.read_csv(\"data/FashionMNIST2/archive/fashion-mnist_test.csv\")\n",
    "train_data = train_data.to_numpy()\n",
    "labels_dataset = []\n",
    "for data in train_data:\n",
    "    label = data[0].item()\n",
    "    image_file_name = labels_map[label]+str(uuid.uuid4())+\".jpg\"\n",
    "    image_path = \"data/FashionMNIST2/test/images/\"+image_file_name\n",
    "    image_data = data[1:].reshape((28,28))\n",
    "    img = Image.fromarray(np.asarray(image_data).astype(np.uint8))\n",
    "    img.save(image_path)\n",
    "    labels_dataset.append([image_file_name,label])\n",
    "\n",
    "labels = pd.DataFrame(labels_dataset)\n",
    "labels.to_csv(\"data/FashionMNIST2/test/labels.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images_directory, targets_file, image_transform=None, label_tranform=None):\n",
    "        self.images_directory = images_directory\n",
    "        self.image_labels = pd.read_csv(targets_file)\n",
    "        self.image_transform = image_transform\n",
    "        self.label_tranform = label_tranform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.images_directory,self.image_labels.iloc[index, 0])\n",
    "        image = read_image(image_path)\n",
    "        label = self.image_labels.iloc[index, 1]\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.label_tranform:\n",
    "            label = self.label_tranform(label)\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(\"data/FashionMNIST2/train/images/\",\"data/FashionMNIST2/train/labels.csv\")\n",
    "test_dataset = CustomImageDataset(\"data/FashionMNIST2/test/images/\",\"data/FashionMNIST2/test/labels.csv\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features[0].shape)\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(\"data/FashionMNIST2/train/images/\",\n",
    "                                   \"data/FashionMNIST2/train/labels.csv\",\n",
    "                                   image_transform=ToTensor(),\n",
    "                                   label_tranform=Lambda(lambda y:torch.zeros(10,dtype=torch.float64).scatter_(0, torch.tensor(6), value=1))\n",
    "                                )\n",
    "test_dataset = CustomImageDataset(\"data/FashionMNIST2/test/images/\",\n",
    "                                    \"data/FashionMNIST2/test/labels.csv\",\n",
    "                                    image_transform=ToTensor(),\n",
    "                                    label_tranform=Lambda(lambda y:torch.zeros(10,dtype=torch.float64).scatter_(0, torch.tensor(6), value=1))\n",
    "                                 )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
